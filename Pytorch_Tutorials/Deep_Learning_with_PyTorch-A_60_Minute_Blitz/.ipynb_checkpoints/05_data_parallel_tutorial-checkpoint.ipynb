{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가자료: 데이터 병렬처리\n",
    "==========================\n",
    "**Authors**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n",
    "\n",
    "In this tutorial, we will learn how to use multiple GPUs using ``DataParallel``.    \n",
    "이 예제에서는 다중 GPU를 사용하는 ``DataParallel`` (데이터 병렬처리)에 대해 배웁니다.\n",
    "\n",
    "It's very easy to use GPUs with PyTorch. You can put the model on a GPU:    \n",
    "PyTorch에서 GPU를 사용하는 것은 매우 쉽습니다. 모델을 GPU에 넣기만 하면 됩니다.    \n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "\n",
    "Then, you can copy all your tensors to the GPU:\n",
    "그런 다음, 모든 텐서를 GPU에 복사할 수 있습니다.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    mytensor = my_tensor.to(device)\n",
    "\n",
    "Please note that just calling ``my_tensor.to(device)`` returns a new copy of\n",
    "``my_tensor`` on GPU instead of rewriting ``my_tensor``. You need to assign it to\n",
    "a new tensor and use that tensor on the GPU.\n",
    "\n",
    "``my_tensor.to(device)``를 호출하면, ``my_tensor``를 덮어 쓰는 것이 아니라 GPU를 사용하는 ``my_tensor``의 복사본을 반환합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It's natural to execute your forward, backward propagations on multiple GPUs.        \n",
    "However, Pytorch will only use one GPU by default. You can easily run your\n",
    "operations on multiple GPUs by making your model run parallelly using\n",
    "다중 GPU 환경에서 전파(forward), 역전파(backward)를 진행하는 것이 당연합니다.\n",
    "하지만, Pytorch는 기본적으로 GPU를 하나만 사용합니다. ``DataParallel``로 모델을 병렬로 실행해 주면, 손쉽게 다중 GPU 환경에서 작업할 수 있습니다.\n",
    "``DataParallel``:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "That's the core behind this tutorial. We will explore it in more detail below.\n",
    "이것이 이번 예제의 핵심입니다. 아래에서 더 자세히 살펴보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and parameters\n",
    "----------------------\n",
    "\n",
    "Import PyTorch modules and define parameters.    \n",
    "PyTorch을 import 하고, 매개 변수를 정의합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#GPU 사용 가능한 환경이면, device를 cuda:0 으로 설정한다. 사용 가능한 환경이 아니라면, cpu로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy DataSet\n",
    "-------------\n",
    "\n",
    "Make a dummy (random) dataset. You just need to implement the getitem    \n",
    "더미(무작위) 데이터 셋을 만듭니다. getitem을 구현해야 합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset): #Dataset 상속\n",
    "    #학습할 데이터의 크기가 크지 않으면, 모두 한 번에 불러와서 한 번에 학습시캬도 큰 차이가 없다.\n",
    "    #하지만 데이터가 많아지면, batch로 나눠서 처리해야 한다. Dataset으로 가져올 데이터를 정의해 주고 DataLoader로 가져오면 편하다.\n",
    "    #custom dataLoader를 만들기 위해, 해당 데이터를 Dataset를 상속해 클래스로 구현해 준다. init, getitem, len을 구현해 줘야 한다.\n",
    "\n",
    "    def __init__(self, size, length): #1. download, read data, etc. #생성자 \n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size) #데이터를 정제해 준다. 여기서는 단순히 랜덤\n",
    "\n",
    "    def __getitem__(self, index): #2. return one item on the index #해당 index의 데이터 반환 \n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self): #3. return the data length #데이터의 총 길이\n",
    "        return self.len\n",
    "    \n",
    "    #위 3개의 단계를 거쳐 Datset을 만들어 준다.\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "#Dataset을 만든 후, DataLoader로 가져온다.\n",
    "#batch_size만큼 데이터를 한 번에 가져온다. #num_workers는 사용하는 CPU의 성능과 관련이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Model\n",
    "------------\n",
    "\n",
    "For the demo, our model just gets an input, performs a linear operation, and\n",
    "gives an output. However, you can use ``DataParallel`` on any model (CNN, RNN,\n",
    "Capsule Net etc.)    \n",
    "여기에서는 입력을 받아 선형 연산 후 출력하는 매우 단순한 모델을 만듭니다. 그러나 ``DataParallel``은 어떤 모델에서든 사용할 수 있습니다(CNN, RNN,\n",
    "Capsule Net etc.).\n",
    "\n",
    "We've placed a print statement inside the model to monitor the size of input\n",
    "and output tensors.\n",
    "Please pay attention to what is printed at batch rank 0.    \n",
    "입력과 출력 텐서의 size를 확인하기 위해, 모델에 출력 구문(print)을 씁니다. 첫 번째 batch 출력문에 집중해 주세요.??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): #Module상속\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input): #train, predict. #nn.Module class 에서 반드시 구현해야 한다.\n",
    "        output = self.fc(input)\n",
    "        print(\"\\tIn Model: input size\", input.size(),\n",
    "              \"output size\", output.size())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and DataParallel\n",
    "-----------------------------\n",
    "\n",
    "This is the core part of the tutorial. First, we need to make a model instance\n",
    "and check if we have multiple GPUs. If we have multiple GPUs, we can wrap\n",
    "our model using ``nn.DataParallel``. Then we can put our model on GPUs by\n",
    "``model.to(device)``     \n",
    "이 예제의 핵심 부분입니다. 먼저 모델 객체를 만들고, 다중 GPU를 사용할 수 있는 환경인지 확인합니다. 다중 GPU를 사용할 수 있는 환경이라면, 모델을 ``nn.DataParallel``로 래핑할 수 있습니다. 그 후에, ``model.to(device)``를 사용해 모델을 GPU로 보냅니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1: #GPU를 2개 이상 사용할 수 있는 환경이라면, \n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Model\n",
    "-------------\n",
    "\n",
    "Now we can see the sizes of input and output tensors.    \n",
    "텐서의 입력과 출력 크기를 확인합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
      "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"Outside: input size\", input.size(),\n",
    "          \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "-------\n",
    "\n",
    "If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as\n",
    "expected. But if you have multiple GPUs, then you can get results like this.    \n",
    "GPU가 없거나 하나만 있는 환경이라면, batch_size를 30으로 설정합니다. 예상대로 모델은 30개의 데이터를 한번에 입력 받아 30개의 결과를 출력합니다.  그러나, 다중 GPU 환경이라면, 결과는 아래와 같습니다.\n",
    "\n",
    "2 GPUs\n",
    "~~~~~~\n",
    "\n",
    "If you have 2, you will see:\n",
    "\n",
    ".. code:: bash\n",
    "\n",
    "    # on 2 GPUs\n",
    "    Let's use 2 GPUs!\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "\n",
    "3 GPUs\n",
    "~~~~~~\n",
    "\n",
    "If you have 3 GPUs, you will see:\n",
    "\n",
    ".. code:: bash\n",
    "\n",
    "    Let's use 3 GPUs!\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "\n",
    "8 GPUs\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "If you have 8, you will see:\n",
    "\n",
    ".. code:: bash\n",
    "\n",
    "    Let's use 8 GPUs!\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "-------\n",
    "\n",
    "DataParallel splits your data automatically and sends job orders to multiple\n",
    "models on several GPUs. After each model finishes their job, DataParallel\n",
    "collects and merges the results before returning it to you.    \n",
    "DataParallel은 데이터를 자동으로 분할하고, 다중 GPU 환경에서 모델이 작업하도록 합니다. 각 모델이 작업을 끝내면, DataParallel는 결과를 수집하고 병합하여 반환합니다.\n",
    "\n",
    "For more information, please check out\n",
    "http://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
