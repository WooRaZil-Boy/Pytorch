{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Quadratic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict #이건 Python collections 임\n",
    "\n",
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000 #데이터 1000개\n",
    "num_epochs = 5000 #epoch 5000번\n",
    "\n",
    "noise = init.normal_(torch.FloatTensor(num_data, 1), std=1)\n",
    "#표준편차가 1의, num_data x 1 size의 FloatTensor 생성. 정규분포\n",
    "\n",
    "x = init.uniform_(torch.Tensor(num_data, 1), -15, 15)\n",
    "#-10은 균등 분포 최소 값, 10은 균등 분포 최대 값. 균등분포는 모든 요소 확률 같음\n",
    "y = (x**2) + 3 #구하려는 식\n",
    "\n",
    "y_noise = y + noise #실제 데이터로 주어지는 식은 noise가 끼어 있어 제대로 된 방정식을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            (\"fc1\", nn.Linear(input_dim, 6)), #Linear 모델 (in_features, out_features, bias=True)\n",
    "            (\"relu1\", nn.ReLU()),\n",
    "            (\"fc2\", nn.Linear(6, 10)),\n",
    "            (\"relu2\", nn.ReLU()),\n",
    "            (\"fc3\", nn.Linear(10, 6)),\n",
    "            (\"relu3\", nn.ReLU()),\n",
    "            (\"fc4\", nn.Linear(6, output_dim))\n",
    "        ]))\n",
    "        #Sequential는 컨테이너로 순서대로 추가된다. 여러 개의 레이어를 합쳐 하나의 이름으로 묶을 수 있다.\n",
    "        #OrderedDict를 사용하면, 개별 레이어와 작업의 이름을 지정해 줄 수 있다. 딕셔너리이므로 key가 중복되어선 안된다.\n",
    "        #https://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
    "        \n",
    "        #activate function이 없으면, neural network라도 linear하게 결과를 낼 수 밖에 없다.\n",
    "        #여기 예에서는 activate function이 없으면, 2차원 함수라도 1차원으로 선을 그어 판단하게 된다.\n",
    "        #하지만, activate function이 추가되면, 데이터의 분포에 맞춰 곡선으로 그어질 수 있다.\n",
    "        #Relu 없이 학습해보면 loss 차이를 확인해 볼 수 있다.\n",
    "    \n",
    "    def forward(self, x): #x가 Tensor\n",
    "        return self.model(x)\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, \tloss 77.71380615234375\n",
      "epoch 200, \tloss 77.38748931884766\n",
      "epoch 400, \tloss 77.10857391357422\n",
      "epoch 600, \tloss 76.8351058959961\n",
      "epoch 800, \tloss 76.47795867919922\n",
      "epoch 1000, \tloss 75.77306365966797\n",
      "epoch 1200, \tloss 70.74507904052734\n",
      "epoch 1400, \tloss 21.627145767211914\n",
      "epoch 1600, \tloss 18.98366928100586\n",
      "epoch 1800, \tloss 15.992815017700195\n",
      "epoch 2000, \tloss 11.917135238647461\n",
      "epoch 2200, \tloss 7.400510787963867\n",
      "epoch 2400, \tloss 7.688417911529541\n",
      "epoch 2600, \tloss 7.0035319328308105\n",
      "epoch 2800, \tloss 6.158647537231445\n",
      "epoch 3000, \tloss 5.506040096282959\n",
      "epoch 3200, \tloss 5.888855457305908\n",
      "epoch 3400, \tloss 5.542513370513916\n",
      "epoch 3600, \tloss 6.280086994171143\n",
      "epoch 3800, \tloss 5.962369441986084\n",
      "epoch 4000, \tloss 5.023372173309326\n",
      "epoch 4200, \tloss 4.883553504943848\n",
      "epoch 4400, \tloss 4.692893981933594\n",
      "epoch 4600, \tloss 4.935478687286377\n",
      "epoch 4800, \tloss 4.8362555503845215\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.001\n",
    "criterion = nn.L1Loss() #손실함수\n",
    "optimizer = optim.SGD(model.parameters(), lr=l_rate) #parameters() 메서드로 해당 객체(모델)의 모든 파라미터를 가져온다.\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad() #optimiser.step으로 업데이트된 그라디언트 값들을 초기화해 줘야 한다.\n",
    "    #이 코드가 없으면, 그라디언트 값들이 누적되어서 제대로 학습되지 않는다.\n",
    "    outputs = model.forward(x) #학습\n",
    "    loss = criterion(outputs, y_noise) #예측한 값과, 정답\n",
    "    loss.backward() #역전파 해 준다.\n",
    "    optimizer.step() #변수 업데이트 \n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"epoch {}, \\tloss {}\".format(i, loss)) #200번 마다 손실 출력\n",
    "        \n",
    "#원래는 Tensor 값인 x, y를 Variable로 변환해서 traininig 해야 했지만, 4.0 부터는 Tensor 그대로 넣어도 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:turienv]",
   "language": "python",
   "name": "conda-env-turienv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
