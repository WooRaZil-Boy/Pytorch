{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Cubic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict #이건 Python collections 임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000 #데이터 1000개\n",
    "num_epochs = 5000 #epoch 5000번\n",
    "\n",
    "x = init.uniform_(torch.Tensor(num_data, 1), -10, 10)\n",
    "#-10은 균등 분포 최소 값, 10은 균등 분포 최대 값. 균등분포는 모든 요소 확률 같음\n",
    "y = (x**3) - 3*(x**2) - 9*x -1\n",
    "\n",
    "noise = init.normal_(torch.FloatTensor(num_data, 1), std=0.5)\n",
    "#표준편차가 0.5의, num_data x 1 size의 FloatTensor 생성. 정규분포\n",
    "\n",
    "y_noise = y + noise #실제 데이터로 주어지는 식은 noise가 끼어 있어 제대로 된 방정식을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_num=1, output_num=1):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            (\"fc1\", nn.Linear(input_num, 20)), #Linear 모델 (in_features, out_features, bias=True)\n",
    "            (\"relu1\", nn.ReLU()),\n",
    "            (\"fc2\", nn.Linear(20, 10)), \n",
    "            (\"relu2\", nn.ReLU()),\n",
    "            (\"fc3\", nn.Linear(10, 5)), \n",
    "            (\"relu3\", nn.ReLU()),\n",
    "            (\"fc4\", nn.Linear(5, output_num))\n",
    "        ]))\n",
    "        \n",
    "        #Sequential는 컨테이너로 순서대로 추가된다. 여러 개의 레이어를 합쳐 하나의 이름으로 묶을 수 있다.\n",
    "        #OrderedDict를 사용하면, 개별 레이어와 작업의 이름을 지정해 줄 수 있다. 딕셔너리이므로 key가 중복되어선 안된다.\n",
    "        #https://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
    "        \n",
    "        #activate function이 없으면, neural network라도 linear하게 결과를 낼 수 밖에 없다.\n",
    "        #여기 예에서는 activate function이 없으면, 2차원 함수라도 1차원으로 선을 그어 판단하게 된다.\n",
    "        #하지만, activate function이 추가되면, 데이터의 분포에 맞춰 곡선으로 그어질 수 있다.\n",
    "        #Relu 없이 학습해보면 loss 차이를 확인해 볼 수 있다.\n",
    "        \n",
    "    def forward(self, x): #x가 Tensor\n",
    "        return self.model(x)\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, \tloss 219.978271484375\n",
      "epoch 200, \tloss 218.924072265625\n",
      "epoch 400, \tloss 201.21429443359375\n",
      "epoch 600, \tloss 133.3739776611328\n",
      "epoch 800, \tloss 125.23332977294922\n",
      "epoch 1000, \tloss 105.49205780029297\n",
      "epoch 1200, \tloss 59.75245666503906\n",
      "epoch 1400, \tloss 47.7353630065918\n",
      "epoch 1600, \tloss 39.92662048339844\n",
      "epoch 1800, \tloss 50.334495544433594\n",
      "epoch 2000, \tloss 28.401138305664062\n",
      "epoch 2200, \tloss 33.611331939697266\n",
      "epoch 2400, \tloss 21.707387924194336\n",
      "epoch 2600, \tloss 41.90111541748047\n",
      "epoch 2800, \tloss 36.4487190246582\n",
      "epoch 3000, \tloss 23.176036834716797\n",
      "epoch 3200, \tloss 32.807003021240234\n",
      "epoch 3400, \tloss 24.50848960876465\n",
      "epoch 3600, \tloss 28.645904541015625\n",
      "epoch 3800, \tloss 18.315731048583984\n",
      "epoch 4000, \tloss 17.272323608398438\n",
      "epoch 4200, \tloss 25.647497177124023\n",
      "epoch 4400, \tloss 15.191964149475098\n",
      "epoch 4600, \tloss 15.13269329071045\n",
      "epoch 4800, \tloss 16.005477905273438\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.001\n",
    "criterion = nn.L1Loss() #손실함수\n",
    "optimizer = optim.SGD(model.parameters(), lr=l_rate) #parameters() 메서드로 해당 객체(모델)의 모든 파라미터를 가져온다.\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad() #optimiser.step으로 업데이트된 그라디언트 값들을 초기화해 줘야 한다.\n",
    "    outputs = model.forward(x) #학습\n",
    "    loss = criterion(outputs, y_noise) #예측한 값과, 정답\n",
    "    loss.backward() #역전파 해 준다.\n",
    "    optimizer.step() #변수 업데이트\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"epoch {}, \\tloss {}\".format(i, loss)) \n",
    "#원래는 Tensor 값인 x, y를 Variable로 변환해서 traininig 해야 했지만, 4.0 부터는 Tensor 그대로 넣어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:turienv]",
   "language": "python",
   "name": "conda-env-turienv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
