{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss는 어떻게 생기나?\n",
    "\n",
    "- input(입력) : x\n",
    "- ouput(출력) : y\n",
    "- Label(정답) : d\n",
    "\n",
    "우리는 입력(x)과 정답(d)만을 알고 있고, 모델은 입력(x)을 받아서 출력(y)을 만든다.    \n",
    "$y = Wx + b$    \n",
    "여기서 목적은  $d-y$가 0이 되는 것이다. 따라서 $d-y$가 0이 아닐 때에는 error가 있는 것으로 볼 수 있고 이것을 loss라 할 수 있다.\n",
    "\n",
    "## Loss를 어떻게 계산하는가\n",
    "\n",
    "\n",
    "| Type | Activation Function| Loss Function |\n",
    "|------|--------------------|---------------|\n",
    "|  Regression                  | 항등 사항                         |Squared Error(평균제곱오차|\n",
    "|  Binary Classification       |  Logistic Function(ex. Sigmoid) | Binary Cross Entropy  |\n",
    "|  Multi Class Classification  | Softmax Function                |Cross Entropy          |\n",
    "\n",
    "- Sigmoid는 0 ~ 1 사이의 값이 나온다. 0.5 이상이면 T. 아니면 F로 판단할 수 있다.\n",
    "- Softmax Function 은 출력의 합이 1이 나오므로 확률로 판단할 수 있다.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Gradient Descent Method : 경사 하강법\n",
    "\n",
    "## 순서\n",
    "- 라이브러리 import\n",
    "- Dataset 만들기\n",
    "- Model 만들기\n",
    "- Optimizer와 loss 계산 함수 결정\n",
    "- 학습을 위한 반복문\n",
    "- 평가 및 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), #가져온 이미지를 Tensor 형테로 변환(np를 Tensor로) 채널이 변한다.\n",
    "                                #ToTensor C x H x W 을 H x W x C로 바꿔준다.\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "#transforms은 선처리를 위한 함수. #CenterCrop 등 다른 함수도 있다.\n",
    "#transforms.Compose는 선처리를 위한 컨테이너\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                       train=True, #True로 하면 train 데이터로 가져온다.\n",
    "                                       download=True,\n",
    "                                       transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                       train=False, #false로 하면 test 데이터로 가져온다.\n",
    "                                       download=True,\n",
    "                                       transform=transform)\n",
    "#이미 선언되어 있는 데이터 셋을 가져온다. .을 붙여줘야 된다(안 붙이면 pemission denied).\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=8, shuffle=True, num_workers=2)\n",
    "#num_workers는 CPU 사용 설정. batch_size가 클 수록 num_workers가 큰 게 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/anaconda/envs/turienv/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader) #iter로 순차적으로 하나씩 가져올 수 있다.\n",
    "images, labels = dataiter.next() #next로 다음 아이템으로 넘어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 30, 5)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer와 loss 계산 함수 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9) #momentum은 한 번씩 다른 방향으로 튀는 계수\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 위한 반복문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => loss : 2.311\n",
      "64 => loss : 2.323\n",
      "128 => loss : 2.297\n",
      "192 => loss : 2.256\n",
      "256 => loss : 2.297\n",
      "320 => loss : 2.209\n",
      "384 => loss : 2.183\n",
      "448 => loss : 2.007\n",
      "512 => loss : 1.968\n",
      "576 => loss : 2.275\n",
      "640 => loss : 1.856\n",
      "704 => loss : 1.926\n",
      "768 => loss : 2.099\n",
      "832 => loss : 1.946\n",
      "896 => loss : 1.991\n",
      "960 => loss : 1.921\n",
      "1024 => loss : 1.448\n",
      "1088 => loss : 1.461\n",
      "1152 => loss : 1.634\n",
      "1216 => loss : 1.477\n",
      "1280 => loss : 1.571\n",
      "1344 => loss : 1.667\n",
      "1408 => loss : 1.692\n",
      "1472 => loss : 1.113\n",
      "1536 => loss : 2.463\n",
      "1600 => loss : 1.980\n",
      "1664 => loss : 2.016\n",
      "1728 => loss : 1.642\n",
      "1792 => loss : 2.230\n",
      "1856 => loss : 1.689\n",
      "1920 => loss : 1.322\n",
      "1984 => loss : 2.308\n",
      "2048 => loss : 1.687\n",
      "2112 => loss : 1.453\n",
      "2176 => loss : 1.119\n",
      "2240 => loss : 1.253\n",
      "2304 => loss : 1.415\n",
      "2368 => loss : 1.735\n",
      "2432 => loss : 1.123\n",
      "2496 => loss : 1.725\n",
      "2560 => loss : 2.288\n",
      "2624 => loss : 1.436\n",
      "2688 => loss : 1.591\n",
      "2752 => loss : 1.390\n",
      "2816 => loss : 1.329\n",
      "2880 => loss : 1.303\n",
      "2944 => loss : 1.554\n",
      "3008 => loss : 0.998\n",
      "3072 => loss : 1.190\n",
      "3136 => loss : 1.660\n",
      "3200 => loss : 1.374\n",
      "3264 => loss : 1.494\n",
      "3328 => loss : 1.601\n",
      "3392 => loss : 1.347\n",
      "3456 => loss : 1.647\n",
      "3520 => loss : 1.215\n",
      "3584 => loss : 1.550\n",
      "3648 => loss : 2.296\n",
      "3712 => loss : 1.505\n",
      "3776 => loss : 1.541\n",
      "3840 => loss : 1.257\n",
      "3904 => loss : 1.812\n",
      "3968 => loss : 1.499\n",
      "4032 => loss : 1.217\n",
      "4096 => loss : 1.375\n",
      "4160 => loss : 1.224\n",
      "4224 => loss : 1.918\n",
      "4288 => loss : 1.980\n",
      "4352 => loss : 1.387\n",
      "4416 => loss : 1.331\n",
      "4480 => loss : 1.921\n",
      "4544 => loss : 1.951\n",
      "4608 => loss : 2.155\n",
      "4672 => loss : 1.170\n",
      "4736 => loss : 1.706\n",
      "4800 => loss : 1.545\n",
      "4864 => loss : 2.189\n",
      "4928 => loss : 1.235\n",
      "4992 => loss : 1.272\n",
      "5056 => loss : 2.378\n",
      "5120 => loss : 1.282\n",
      "5184 => loss : 1.506\n",
      "5248 => loss : 0.911\n",
      "5312 => loss : 1.877\n",
      "5376 => loss : 1.335\n",
      "5440 => loss : 1.491\n",
      "5504 => loss : 1.603\n",
      "5568 => loss : 2.047\n",
      "5632 => loss : 1.455\n",
      "5696 => loss : 1.873\n",
      "5760 => loss : 1.779\n",
      "5824 => loss : 0.898\n",
      "5888 => loss : 0.742\n",
      "5952 => loss : 0.736\n",
      "6016 => loss : 1.604\n",
      "6080 => loss : 1.544\n",
      "6144 => loss : 1.361\n",
      "6208 => loss : 1.648\n",
      "0 => loss : 1.350\n",
      "64 => loss : 1.099\n",
      "128 => loss : 1.754\n",
      "192 => loss : 1.374\n",
      "256 => loss : 1.304\n",
      "320 => loss : 1.654\n",
      "384 => loss : 1.175\n",
      "448 => loss : 1.548\n",
      "512 => loss : 1.204\n",
      "576 => loss : 1.972\n",
      "640 => loss : 1.770\n",
      "704 => loss : 0.741\n",
      "768 => loss : 1.098\n",
      "832 => loss : 1.578\n",
      "896 => loss : 1.579\n",
      "960 => loss : 1.925\n",
      "1024 => loss : 1.385\n",
      "1088 => loss : 1.046\n",
      "1152 => loss : 1.222\n",
      "1216 => loss : 1.780\n",
      "1280 => loss : 2.108\n",
      "1344 => loss : 1.275\n",
      "1408 => loss : 1.921\n",
      "1472 => loss : 1.174\n",
      "1536 => loss : 0.764\n",
      "1600 => loss : 1.113\n",
      "1664 => loss : 1.311\n",
      "1728 => loss : 0.704\n",
      "1792 => loss : 0.993\n",
      "1856 => loss : 2.047\n",
      "1920 => loss : 1.218\n",
      "1984 => loss : 1.138\n",
      "2048 => loss : 1.089\n",
      "2112 => loss : 1.149\n",
      "2176 => loss : 1.197\n",
      "2240 => loss : 0.974\n",
      "2304 => loss : 1.166\n",
      "2368 => loss : 0.815\n",
      "2432 => loss : 1.577\n",
      "2496 => loss : 0.899\n",
      "2560 => loss : 1.679\n",
      "2624 => loss : 0.630\n",
      "2688 => loss : 1.192\n",
      "2752 => loss : 0.578\n",
      "2816 => loss : 1.531\n",
      "2880 => loss : 0.617\n",
      "2944 => loss : 0.702\n",
      "3008 => loss : 1.677\n",
      "3072 => loss : 1.227\n",
      "3136 => loss : 1.054\n",
      "3200 => loss : 1.339\n",
      "3264 => loss : 0.855\n",
      "3328 => loss : 0.943\n",
      "3392 => loss : 1.733\n",
      "3456 => loss : 1.153\n",
      "3520 => loss : 1.452\n",
      "3584 => loss : 2.048\n",
      "3648 => loss : 1.060\n",
      "3712 => loss : 0.784\n",
      "3776 => loss : 1.359\n",
      "3840 => loss : 1.152\n",
      "3904 => loss : 0.723\n",
      "3968 => loss : 0.609\n",
      "4032 => loss : 1.034\n",
      "4096 => loss : 1.501\n",
      "4160 => loss : 1.906\n",
      "4224 => loss : 0.825\n",
      "4288 => loss : 0.827\n",
      "4352 => loss : 0.763\n",
      "4416 => loss : 0.724\n",
      "4480 => loss : 0.625\n",
      "4544 => loss : 1.229\n",
      "4608 => loss : 1.796\n",
      "4672 => loss : 1.639\n",
      "4736 => loss : 0.912\n",
      "4800 => loss : 1.752\n",
      "4864 => loss : 0.878\n",
      "4928 => loss : 0.873\n",
      "4992 => loss : 1.082\n",
      "5056 => loss : 0.990\n",
      "5120 => loss : 1.446\n",
      "5184 => loss : 1.271\n",
      "5248 => loss : 1.435\n",
      "5312 => loss : 1.022\n",
      "5376 => loss : 1.142\n",
      "5440 => loss : 0.914\n",
      "5504 => loss : 1.115\n",
      "5568 => loss : 1.786\n",
      "5632 => loss : 1.553\n",
      "5696 => loss : 1.132\n",
      "5760 => loss : 0.938\n",
      "5824 => loss : 0.984\n",
      "5888 => loss : 0.997\n",
      "5952 => loss : 0.737\n",
      "6016 => loss : 1.370\n",
      "6080 => loss : 1.223\n",
      "6144 => loss : 2.354\n",
      "6208 => loss : 1.777\n",
      "0 => loss : 0.864\n",
      "64 => loss : 0.611\n",
      "128 => loss : 1.340\n",
      "192 => loss : 1.241\n",
      "256 => loss : 0.941\n",
      "320 => loss : 0.724\n",
      "384 => loss : 1.450\n",
      "448 => loss : 1.261\n",
      "512 => loss : 1.519\n",
      "576 => loss : 1.057\n",
      "640 => loss : 1.184\n",
      "704 => loss : 0.708\n",
      "768 => loss : 0.792\n",
      "832 => loss : 1.039\n",
      "896 => loss : 0.907\n",
      "960 => loss : 0.412\n",
      "1024 => loss : 1.209\n",
      "1088 => loss : 1.159\n",
      "1152 => loss : 1.761\n",
      "1216 => loss : 0.512\n",
      "1280 => loss : 0.664\n",
      "1344 => loss : 0.578\n",
      "1408 => loss : 0.716\n",
      "1472 => loss : 0.452\n",
      "1536 => loss : 1.063\n",
      "1600 => loss : 0.982\n",
      "1664 => loss : 2.030\n",
      "1728 => loss : 0.849\n",
      "1792 => loss : 1.144\n",
      "1856 => loss : 1.450\n",
      "1920 => loss : 0.946\n",
      "1984 => loss : 0.906\n",
      "2048 => loss : 1.248\n",
      "2112 => loss : 0.618\n",
      "2176 => loss : 1.130\n",
      "2240 => loss : 1.056\n",
      "2304 => loss : 1.364\n",
      "2368 => loss : 0.402\n",
      "2432 => loss : 1.034\n",
      "2496 => loss : 0.989\n",
      "2560 => loss : 1.721\n",
      "2624 => loss : 1.666\n",
      "2688 => loss : 0.378\n",
      "2752 => loss : 1.439\n",
      "2816 => loss : 0.770\n",
      "2880 => loss : 1.112\n",
      "2944 => loss : 1.125\n",
      "3008 => loss : 1.064\n",
      "3072 => loss : 0.800\n",
      "3136 => loss : 1.385\n",
      "3200 => loss : 0.804\n",
      "3264 => loss : 0.627\n",
      "3328 => loss : 0.559\n",
      "3392 => loss : 1.390\n",
      "3456 => loss : 1.039\n",
      "3520 => loss : 0.904\n",
      "3584 => loss : 0.851\n",
      "3648 => loss : 0.971\n",
      "3712 => loss : 0.795\n",
      "3776 => loss : 0.934\n",
      "3840 => loss : 1.238\n",
      "3904 => loss : 1.215\n",
      "3968 => loss : 1.495\n",
      "4032 => loss : 0.403\n",
      "4096 => loss : 1.669\n",
      "4160 => loss : 1.049\n",
      "4224 => loss : 1.798\n",
      "4288 => loss : 1.315\n",
      "4352 => loss : 1.165\n",
      "4416 => loss : 1.191\n",
      "4480 => loss : 0.953\n",
      "4544 => loss : 0.973\n",
      "4608 => loss : 0.601\n",
      "4672 => loss : 0.792\n",
      "4736 => loss : 2.058\n",
      "4800 => loss : 0.657\n",
      "4864 => loss : 2.739\n",
      "4928 => loss : 0.891\n",
      "4992 => loss : 0.525\n",
      "5056 => loss : 0.626\n",
      "5120 => loss : 0.888\n",
      "5184 => loss : 0.864\n",
      "5248 => loss : 0.661\n",
      "5312 => loss : 0.488\n",
      "5376 => loss : 1.857\n",
      "5440 => loss : 1.022\n",
      "5504 => loss : 0.253\n",
      "5568 => loss : 0.822\n",
      "5632 => loss : 0.992\n",
      "5696 => loss : 0.873\n",
      "5760 => loss : 0.675\n",
      "5824 => loss : 1.102\n",
      "5888 => loss : 1.469\n",
      "5952 => loss : 0.870\n",
      "6016 => loss : 0.626\n",
      "6080 => loss : 0.677\n",
      "6144 => loss : 0.899\n",
      "6208 => loss : 1.350\n",
      "train over\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 3\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = loss_function(out, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i % 64 == 0:\n",
    "            print(\"%d => loss : %.3f\"%(i, loss))\n",
    "            \n",
    "print(\"train over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 65.000000\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1) #예측한 클래스의 확률 중 가장 큰 값을 가져온다.\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted==labels).sum()\n",
    "    \n",
    "print(\"Accuracy of the network on the 10000 test images: %f\"%(100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential을 사용할 때 Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1) #class를 따로 만들어 view를 해주면 된다.\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(3, 64, 5)), \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"pool1\", nn.MaxPool2d(2, 2)),\n",
    "            (\"conv2\", nn.Conv2d(64, 30, 5)),\n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"pool2\", nn.MaxPool2d(2, 2)),\n",
    "            (\"flatten\", Flatten()),\n",
    "            (\"fc1\", nn.Linear(30*5*5, 128)), \n",
    "            (\"relu3\", nn.ReLU(inplace=True)),\n",
    "            (\"fc2\", nn.Linear(128, 10)),\n",
    "            (\"relu4\", nn.ReLU(inplace=True))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9) #momentum은 한 번씩 다른 방향으로 튀는 계수\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_num = 3\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        out = model(inputs)\n",
    "        \n",
    "        loss = loss_function(out, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i % 64 == 0:\n",
    "            print(\"%d => loss : %.3f\"%(i, loss))\n",
    "            \n",
    "print(\"train over\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:turienv]",
   "language": "python",
   "name": "conda-env-turienv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
